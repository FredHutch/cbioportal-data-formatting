{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaisisImport. A utility to prepare a Caisis-to-Excel export for import into Oncoscape (via cBioPortal format).\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Brain_March2022'\n",
    "# diseaseChosen = 'Brain'  \n",
    "# study_identifier = 'brain_abc'  #used for copying brain_abc_custom_caisis_prep.py to this folder as custom_caisis_prep.py.\n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Sarcoma_March2022'   \n",
    "# diseaseChosen = 'Sarcoma'  #'Brain'\n",
    "# study_identifier = 'sarcoma_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Breast_March2022'   \n",
    "# diseaseChosen = 'Breast Cancer'  #'Brain'\n",
    "# study_identifier = 'breast_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Liver_March2022'   \n",
    "# diseaseChosen = 'Liver Cancer'   \n",
    "# study_identifier = 'liver_caisis' \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Pancreas_March2022'   \n",
    "# diseaseChosen = 'Pancreas Cancer'  #'Brain'\n",
    "# study_identifier = 'pancreas_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Lymphoma_March2022'   \n",
    "# diseaseChosen = 'Lymphoma'  #'Brain'\n",
    "# study_identifier = 'lymphoma_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "dataset_folder = '../Caisis_NonPublicData/Meningioma_Holland'   \n",
    "diseaseChosen = 'Brain'  # really Meningioma specifically, but Caisis has it as Brain.\n",
    "study_identifier = 'Meningioma_caisis'  \n",
    "type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Steps:\n",
    "# . Limit by disease\n",
    "# . Read in CSVs\n",
    "# . Export as TSVs\n",
    "# . ZeroDates\n",
    "# . \n",
    "\n",
    "# oncoscape_bar_override: {\"version\": \"1.0\", \"style\": \"Symbols\", \"shape\": \"circle\", \"subtypeColors\": {\"rp\": \"#FF0000\", \"xrt\": \"#00FF00\"}}\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import math\n",
    "import shutil\n",
    "import pipes\n",
    "import re\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from os.path import exists \n",
    "\n",
    "from zero_dates import   zero_dates\n",
    "\n",
    "global custom_prep_filename\n",
    "global global_subtype_loop_row\n",
    "\n",
    "#===============================================================\n",
    "custom_prep_fullpath = os.path.join(\".\", study_identifier +\"_custom_caisis_prep.py\") # Can override this to any location you like.\n",
    "study_description = study_identifier + ' study'\n",
    "\n",
    "## knownTables = ['Demographics', 'Encounters', 'ClinicalStage', 'LabTests', 'PathTest', 'SocialHistory', 'LabTestGenetics', 'PathStageGrade', 'RadiationTherapy',  ]\n",
    "# table Status is treated separately, first.\n",
    "knownTables = ['Clinical Stages', 'Demographics', 'Encounters', 'Medical Therapy', 'Pathology', 'Procedures', 'Radiation Therapy', 'Social History', 'Status', 'Clinical Stages']\n",
    "date_format = '%Y-%m-%d'  # e.g. 2021-01-23\n",
    "\n",
    "#days_before_dx_to_include = 0  # if 30, can include events up to 30 days before the zero date. Default is 30.\n",
    "global missing_date_str\n",
    "missing_date_str = '2222-02-02'  # use instead of empty/None to indicate missing data\n",
    "\n",
    "missing_date = datetime.strptime(missing_date_str, date_format)  #datetime(2222, 2,2)  # TBD: \n",
    "\n",
    "\n",
    "## -- internal initialization --\n",
    "patients_first_dx = pd.DataFrame()   # just PatientId and DiagnosisDate\n",
    "data_clinical_patient = pd.DataFrame() # Will become the data_clinical_patient table.\n",
    "loaded_tables_dict = {}\n",
    "data_clinical_patient = None\n",
    "data_clinical_timeline_dfs = {}   # dictionary of dataframes, keyed off of \"timeline-foo\" names.\n",
    "data_clinical_timeline_graph_markers = {}  # dictionary of graph_marker_type values, to define arcs, bars, squares, diamonds, circles, triangles.\n",
    "datafiles_fields = {}\n",
    "current_lookup_field = None  # placeholder.\n",
    "noval_list = []   # List of patient IDs with no associated value in one or more of the import_fields. Use this for reporting. \n",
    "\n",
    "foldername_with_headers = '01_with_headers'\n",
    "foldername_zero_dates = '02_zero_dates'\n",
    "custom_prep_filename = \"custom_caisis_prep.py\"\n",
    "has_custom_prep_file = False\n",
    "# src_path = dataset_folder\n",
    "# dst = \".\"\n",
    "\n",
    "file_path = custom_prep_fullpath \n",
    "if  exists(file_path):\n",
    "    print(\"== custom prep exists.\")\n",
    "    shutil.copy(file_path, os.path.join(\".\", custom_prep_filename))\n",
    "    has_custom_prep_file = True\n",
    "    if 'custom_caisis_prep' in sys.modules.keys():\n",
    "        print(\"====================prepmod exists   \")\n",
    "        del sys.modules['custom_caisis_prep']\n",
    "    import custom_caisis_prep\n",
    "    print(\"testing...\")\n",
    "    print(custom_caisis_prep.version)\n",
    "else:\n",
    "    print(\"== no custom prep exists.\")\n",
    "    if  exists(\"./\"+custom_prep_filename):\n",
    "        os.remove(\"./\"+custom_prep_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_marker_types = {\n",
    "    \"arcs\"    : { \"mtype\": \"Arcs\", \"shape\":None},\n",
    "    \"bars\"    : { \"mtype\": \"Bars\", \"shape\":None},\n",
    "    \"circles\" : { \"mtype\": \"Symbols\", \"shape\":\"circle\"},\n",
    "    \"squares\" : { \"mtype\": \"Symbols\", \"shape\":\"square\"},\n",
    "    \"triangles\": { \"mtype\": \"Symbols\", \"shape\":\"triangle\"},\n",
    "    \"diamonds\" : { \"mtype\": \"Symbols\", \"shape\":\"diamond\"},\n",
    "    \"stars\"    : { \"mtype\": \"Symbols\", \"shape\":\"star\"},  # Not yet implemented\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportField:\n",
    "    source_name:str\n",
    "    final_name:str\n",
    "    type:str = 'STRING'  # STRING, NUMBER, or DATE (DATE gets turned into STING in final TSV files.)\n",
    "    conversion_function = None\n",
    "\n",
    "    def __init__(self, source_name, final_name, type=\"STRING\", conversion_function=None ):\n",
    "        self.source_name = source_name\n",
    "        self.final_name = final_name\n",
    "        self.type = type\n",
    "        self.conversion_function = conversion_function\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.source_name +\"->\"+ self.final_name+\", type=\"+self.type+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_timeline_fields = [\n",
    "    ImportField('EVENT_TYPE', final_name='EVENT_TYPE') , \n",
    "    ImportField('START_DATE', final_name='START_DATE', type='DATE') , \n",
    "    ImportField('STOP_DATE', final_name='STOP_DATE', type='DATE') , \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_and_folder(filename, obj):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(obj)\n",
    "\n",
    "def patients_and_descriptive_header(df:pd.DataFrame, header):\n",
    "    # TBD: note header in separate place\n",
    "    str_ids = df.to_string(index=False)\n",
    "    a =  str_ids  # header + \"\\n\" + str_ids\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patients_first_dx():\n",
    "    global patients_first_dx, data_clinical_patient, dataset_folder\n",
    "    print(\"\\nSTEP 1: Find patients' first diagnosis date.\")\n",
    "    print(dataset_folder)\n",
    "    status_fullpath = dataset_folder +'/raw_csv/Status.csv'\n",
    "    if not os.path.exists(dataset_folder +'/raw_csv'):\n",
    "        sys.exit('ERROR: Cannot find folder \"raw_csv\".')\n",
    "\n",
    "    if not os.path.exists(status_fullpath):\n",
    "        sys.exit('ERROR: Status.csv file not found at ' + status_fullpath)\n",
    "\n",
    "    tbl_status = pd.read_csv(status_fullpath) \n",
    "    tbl_status['Date'] =pd.to_datetime(tbl_status.StatusDate)\n",
    "    tbl_status['PatientId'] =  tbl_status['PatientId'].astype(str)\n",
    "\n",
    "    dxRows = tbl_status[tbl_status.Status.eq('Diagnosis Date')]\n",
    "    print('## dxRows = ' + str(len(dxRows)))   \n",
    "    diseaseRows = dxRows[dxRows.StatusDisease.eq(diseaseChosen)].sort_values('Date')\n",
    "    print('## diseaseRows = ' + str(len(diseaseRows)))   \n",
    "\n",
    "    dxNanRows = diseaseRows[diseaseRows.Date.isna()][['PatientId']]\n",
    "    print('## dxNanRows = ' + str(len(dxNanRows)))   \n",
    "    \n",
    "    if(dxNanRows.size > 0):\n",
    "        print('CHECK NoDxDate? REPORT NoDxDate.txt has *' +str(dxNanRows.size)+'* diagnoses of '+diseaseChosen+' without diagnosis dates.')\n",
    "        report_body = patients_and_descriptive_header(dxNanRows, 'Patients with \"'+diseaseChosen+\" but no DiagnosisDate:\")\n",
    "    #    save_file_and_folder('reports/NoDxDate.txt', report_body)\n",
    "        save_file_and_folder(dataset_folder+'/reports/NoDxDate.txt', report_body)\n",
    "    else:\n",
    "        print('CHECK NoDxDate? OK')\n",
    "        print('TBD: delete existing NoDxDate.txt report.')\n",
    "        if os.path.exists(dataset_folder+'/reports/NoDxDate.txt'):\n",
    "            os.remove(dataset_folder+'/reports/NoDxDate.txt')\n",
    "            print('REMOVED NoDxDate.txt')\n",
    "            \n",
    "    diseaseDatedRows = diseaseRows[diseaseRows.Date.isna()==False]\n",
    "    print('## diseaseDatedRows = ' + str(len(diseaseDatedRows)))   \n",
    "    \n",
    "    patientid_date_dict = {}\n",
    "    patientid_probability_dict = {}\n",
    "    print(\"===START PATIENT LOOP===\")\n",
    "    for index, row in diseaseDatedRows.iterrows():\n",
    "        print(row)\n",
    "        pid = str(row['PatientId'])\n",
    "        if((pid in patientid_date_dict) == False):\n",
    "            justYMD = row['Date']  #datetime.strftime(row['Date'], \"%Y-%m-%d\")\n",
    "            patientid_date_dict[pid] = justYMD\n",
    "            patientid_probability_dict[pid] = row['StatusProbability']\n",
    "        else:\n",
    "            pass\n",
    "    print(\"===END PATIENT LOOP===\")\n",
    "\n",
    "    print('Resulting patient IDs = ' + str(len(patientid_date_dict)))   \n",
    "\n",
    "    data = []\n",
    "    for key in patientid_date_dict.keys():\n",
    "        new_row = [key, patientid_date_dict[key], patientid_probability_dict[key]]\n",
    "        data.append(new_row)\n",
    "    patients_first_dx = pd.DataFrame(data, columns=[\"PatientId\", \"DiagnosisDate\", \"StatusProbability\"])\n",
    "    data_clinical_patient = patients_first_dx.copy()\n",
    "\n",
    "    report_body = patients_and_descriptive_header(patients_first_dx, 'Patients First Diagnosis Date')\n",
    "    \n",
    "    save_file_and_folder(dataset_folder+'/reports/PatientsFirstDx.txt', report_body)\n",
    "    #patients_first_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tables():\n",
    "    global loaded_tables_dict\n",
    "    print(\"\\nSTEP 2: Load all CSV tables.\")\n",
    "    for tableName in knownTables:\n",
    "        full_path = dataset_folder+'/raw_csv/'+tableName+'.csv'\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"WARN -- missing table \"+tableName)\n",
    "        else:\n",
    "            print('Reading ' + tableName+'.csv...')\n",
    "            df = pd.read_csv(full_path)\n",
    "            df = df.astype({\"PatientId\": str})\n",
    "            #df.set_index('PatientId', inplace=True)\n",
    "            #print(df.head(2))\n",
    "            loaded_tables_dict[tableName] = df\n",
    "            print(tableName +\" read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_date_errors=[]\n",
    "\n",
    "def process_val(val, ifield:ImportField):\n",
    "    global global_date_errors\n",
    "    # print(\"enter process_val, name=\"+ifield.source_name+\",type=\"+ifield.type+\"!\")\n",
    "    if ifield.type=='NUMBER':  # for now, this means just integers\n",
    "        # print(str(val))\n",
    "        try:\n",
    "            val = int(val)\n",
    "            if math.isnan(val):\n",
    "                print(\"nan found\")\n",
    "                val = \"\"\n",
    "            if str(val)==\"nan\":\n",
    "                print(\"nan text found\")\n",
    "                val = \"\"   \n",
    "        except:\n",
    "            val =\"\"\n",
    "    if ifield.type=='DATE':\n",
    "        # print(\"DATE-TEST \"+str(ifield.type)+\", val-type=\" + str(type(val)) + \" val=(\"+str(val)+\")\")\n",
    "        if isinstance(val, str):\n",
    "            val = datetime.strptime(val, date_format)\n",
    "            if str(val) == \"NaT\":  # Not a Time\n",
    "                val = \"\"\n",
    "        else:\n",
    "            val=\"\" # TBD: error reporting\n",
    "            global_date_errors.append(\"Expected string for date in \" +ifield.source_name+\", but isn't a string.\")\n",
    "    try:\n",
    "        #print(\"process_val about to return \"+str(val))\n",
    "        return val\n",
    "    except:\n",
    "        noval_list.append(pid)\n",
    "        return None\n",
    "\n",
    "\n",
    "# imports: a dictionary of column name from the tname table into patient table, where value is a function to convert data\n",
    "def import_to_patient_table(tname, import_fields:List[ImportField]): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list, global_date_errors\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "        global_date_errors = []\n",
    "        for ifield in import_fields:\n",
    "\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                print(\"data_clinical_patient['PatientId']...\")\n",
    "                print(data_clinical_patient['PatientId'])\n",
    "                print(data_clinical_patient['PatientId'].size)\n",
    "                print(\"first patient...\")\n",
    "                print(data_clinical_patient['PatientId'][0])\n",
    "                \n",
    "                data_clinical_patient.insert(1, ifield.final_name, None)\n",
    "               # global current_lookup_field\n",
    "                current_lookup_field= ifield.source_name\n",
    "                print(\"Looking for field \" + tname+\".\"+current_lookup_field)\n",
    "                noval_list.clear()\n",
    "\n",
    "                def get_field_value(pid):\n",
    "                    global current_lookup_field, noval_list\n",
    "                    gg = current_table.loc[current_table['PatientId'] == str(pid)]\n",
    "                    hh = gg[current_lookup_field]\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = hh.iloc[0] \n",
    "                        return process_val(val, ifield)\n",
    "\n",
    "                    except:\n",
    "                        print(\"ERROR \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        typef, value, traceback = sys.exc_info()\n",
    "                        print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "\n",
    "                new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "                if len(noval_list) > 0 :\n",
    "                    percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "                    print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "                    #print(str( len(noval_list) / new_values.shape[0]))\n",
    "\n",
    "\n",
    "                data_clinical_patient[ifield.final_name] = new_values\n",
    "                # print(\"test of top 12 patients...\")\n",
    "                # print(data_clinical_patient.head(12))\n",
    "            else:\n",
    "                print(\"==== ERROR: Three is no column '\"+ifield.source_name+\"' in '\"+tname+\"' table. ====\")\n",
    "        if len(global_date_errors) > 0:\n",
    "            print(\"WARN: Table \"+ tname + \" has \" + str(len(global_date_errors))+  \"date errors.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add patient's DEATHDATE as a \"Death\" event. If NaT|<empty str>|-1234, do not add.\n",
    "def import_status_to_event_table_from_patient(event_table_name, tname, import_fields:List[ImportField], event_type=\"EVENT\", \n",
    "    start_date_col_name=None, stop_date_col_name=None, subtype_source=None, patient_column=\"unknown\", final_name=\"unknown\"):\n",
    "\n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list, has_custom_prep_file\n",
    "    global global_subtype_loop_row\n",
    "\n",
    "    print(\"Add patient's \" + patient_column+ \" as a \"+ final_name +\" event.....\")\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        rows_to_save = []\n",
    "        # column_names = [\"PatientId\", \"EVENT_TYPE\", \"START_DATE\", \"STOP_DATE\", \"STATUS\"]\n",
    "        # patient_table = loaded_tables_dict['patient']\n",
    "        # print(\">>> patient tbl...\")\n",
    "        # print(patient_table)\n",
    "\n",
    "        #      current_table = loaded_tables_dict[tname]\n",
    "\n",
    "\n",
    "        # Loop through rows in patient_table_rows_of_dict.\n",
    "        # data_clinical_patient , not patients_first_dx\n",
    "        patient_table_rows_of_dict = data_clinical_patient.to_dict('records')\n",
    "        for row in patient_table_rows_of_dict:\n",
    "            # print(\"patient row for status = \" +str(row))\n",
    "\n",
    "            # common to all events: pid, type, start, and stop.\n",
    "            pid = str(row['PatientId'])\n",
    "            start_date = row[patient_column]\n",
    "            # print(\"patient STARTDATE===== \"+str(start_date))\n",
    "            if(str(start_date)!=\"NaT\" and str(start_date)!=\"\" and str(start_date)!=\"-1234\"  ):\n",
    "                stop_date = start_date\n",
    "\n",
    "                #print('pid='+pid+', etype='+etype+', start='+str(start_date)+',  stop='+str(stop_date)+'.')\n",
    "                \n",
    "                new_row = [pid, \"STATUS\", start_date, stop_date, final_name]\n",
    "                # print(\"new_row= \" +str(new_row))\n",
    "\n",
    "                # print(new_row)\n",
    "                rows_to_save.append(new_row)\n",
    "\n",
    "        # # turn list into df.\n",
    "        # new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "        # data_clinical_patient[ifield.final_name] = new_values\n",
    "        # print(data_clinical_patient.head(12))\n",
    "        print('>>> TBD: append these rows to table.')\n",
    "\n",
    "\n",
    "        # print(type(column_names))\n",
    "        # print(column_names)\n",
    "        # print('---- end of column_names ---')\n",
    "\n",
    "        column_names = column_names = [\"PatientId\", \"EVENT_TYPE\", \"START_DATE\", \"STOP_DATE\", \"STATUS\"]\n",
    "        added_df = pd.DataFrame(rows_to_save, columns=column_names)\n",
    "        df_original = data_clinical_timeline_dfs[event_table_name]\n",
    "        df_new = pd.concat([df_original, added_df], axis=0)\n",
    "        data_clinical_timeline_dfs[event_table_name] = df_new\n",
    "\n",
    "        # data_clinical_timeline_graph_markers[event_table_name] = graph_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clinical_timeline_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# event_table_name: name of table as it will appear in \"data_clinical_<timeline-foo>.txt\" filename. For events, should be \"timeline-foo\".\n",
    "# tname: Name of the dataframe table we have loaded from CSV, which contains these events to import.\n",
    "# import_fields: list of ImportFields OR str. str for case of EVENT_TYPE, which may not exist as column in an event CSV.\n",
    "\n",
    "# event_Type is the name of the subtype column (e.g., RADIATION)\n",
    "# subtype_source is the CSV column with the  source info for the subtype (e.g., RADTXTYPE)\n",
    "# safelist, if not None, is the *only* values acceptable for subtype.\n",
    "# date_lookup_index and date_lookup_table_with_dates are used to look up a date in another table (e.g. in Procedures, based on ProccedureId in first table)\n",
    "\n",
    "def import_to_event_table(event_table_name, tname, import_fields:List[ImportField], event_type=\"EVENT\", \n",
    "    start_date_col_name=None, stop_date_col_name=None, subtype_source=None, graph_marker=None, safelist=None,\n",
    "    date_lookup_index=None, date_lookup_table_with_dates=None, disease_must_match_field=None):\n",
    "\n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list, has_custom_prep_file\n",
    "    global global_subtype_loop_row\n",
    "\n",
    "    print(\"INFO starting to import for event_type of....\")\n",
    "    print(str(event_type))\n",
    "    print(\"... .\")\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        # strategy...\n",
    "        #   a_list = [['dog', 1], ['cat', 2], ['fish', 3]]\n",
    "        #   df = pd.DataFrame(a_list, columns=['animal', 'amount'])\n",
    "\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "\n",
    "        # df_new_events = pd.DataFrame\n",
    "        # if  event_table_name not in data_clinical_timeline_dfs:\n",
    "        #     df_new_events = pd.DataFrame(columns=[\"PatientID\", \"START_DATE\", \"STOP_DATE\", \"EVENT_TYPE\"])\n",
    "        #     data_clinical_timeline_dfs[event_table_name] = df_new_events\n",
    "\n",
    "        these_subtype_rewrites = None  # for typos and cleanup\n",
    "        these_subtype_groupings = None # for bucketing for legend\n",
    "\n",
    "        noval_list.clear()\n",
    "        rows_to_save = []\n",
    "        column_names = [\"PatientId\", \"EVENT_TYPE\", \"START_DATE\", \"STOP_DATE\"]\n",
    "        for ifield in import_fields:\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                column_names.append(ifield.final_name)\n",
    "        if subtype_source is not None:\n",
    "            column_names.append(event_type)  # e.g., \"RADIATION\"\n",
    "            if has_custom_prep_file:\n",
    "\n",
    "                if event_type not in custom_caisis_prep.subtype_rewrites:\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has no subtype rewrite rules.\")\n",
    "                else:\n",
    "                    these_subtype_rewrites = custom_caisis_prep.subtype_rewrites[event_type]\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has \"+ str(len(these_subtype_rewrites)) + \" subtype rewrite rules.\")\n",
    "\n",
    "\n",
    "                \n",
    "                if event_type not in custom_caisis_prep.subtype_groupings:\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has no subtype grouping rules.\")\n",
    "                else:\n",
    "                    these_subtype_groupings = custom_caisis_prep.subtype_groupings[event_type]\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has \"+ str(len(these_subtype_groupings)) + \" subtype grouping rules.\")\n",
    "            else:\n",
    "                print(\"In import_to_event_Table, has_custom_prep_file FALSE\")\n",
    "        print('For ' + event_type+\", rewrite keys:\")\n",
    "        print(str(these_subtype_rewrites))\n",
    "        print('>>>end')\n",
    "\n",
    "        # Loop through rows in current_table.\n",
    "        # Pull out each field from import_fields, and add event type, and start and stop dates.\n",
    "        current_table_rows_of_dict = current_table.to_dict('records')\n",
    "\n",
    "        for row in current_table_rows_of_dict:\n",
    "            etype = None\n",
    "            if event_type in row:  # get event from the event_type column\n",
    "                etype = row[event_type]\n",
    "            else:  # just use event_type as text.\n",
    "                etype = event_type\n",
    "\n",
    "            # common to all events: pid, type, start, and stop.\n",
    "            pid = str(row['PatientId'])\n",
    "            \n",
    "            row_is_good = True\n",
    "                \n",
    "            # ======== Get start and stop dates =======\n",
    "            start_date = missing_date\n",
    "            if start_date_col_name != None:\n",
    "                if date_lookup_index is not None:   # might be a date lookup, like pathology from procedures\n",
    "                    # e.g., table_with_dates=\"Procedures\", date_lookup_table_with_dates=\"ProcedureId\"\n",
    "                    table_with_dates = loaded_tables_dict[date_lookup_table_with_dates]\n",
    "                    date_lookup_index_key = row[date_lookup_index]  #e.g. ProcedureId of 1234\n",
    "                    if (date_lookup_index_key is None) | (str(date_lookup_index_key) =='') | (str(date_lookup_index_key) =='nan') :\n",
    "                        print(\"ERROR: START_DATE missing date_lookup_index_key where pid=\"+pid)\n",
    "\n",
    "                        row_is_good = False\n",
    "                    else:\n",
    "                        gg = table_with_dates.loc[table_with_dates[date_lookup_index] == date_lookup_index_key]\n",
    "                        if gg.empty:\n",
    "                            print(\"empty dataframe for pid '\"+pid+\"'.\")\n",
    "                        else:\n",
    "                            hh = str(gg.iloc[0][start_date_col_name])\n",
    "                            if hh != 'nan':\n",
    "                                start_date = hh            \n",
    "                else:\n",
    "                    start_date = row[start_date_col_name]\n",
    "\n",
    "            stop_date = missing_date\n",
    "            if stop_date_col_name != None:\n",
    "                if date_lookup_index is not None:   # might be a date lookup, like pathology from procedures\n",
    "                    table_with_dates = loaded_tables_dict[date_lookup_table_with_dates]\n",
    "                    date_lookup_index_key = row[date_lookup_index]  #e.g. ProcedureId of 1234\n",
    "                    if (date_lookup_index_key is None) | (str(date_lookup_index_key) =='') | (str(date_lookup_index_key) =='nan') :\n",
    "                        print(\"ERROR: STOP_DATE missing date_lookup_index_key where pid=\"+pid)\n",
    "                        row_is_good = False\n",
    "                    else:\n",
    "                        gg = table_with_dates.loc[table_with_dates[date_lookup_index] == date_lookup_index_key]\n",
    "                        if gg.empty:\n",
    "                            print(\"empty dataframe for pid '\"+pid+\"'.\")\n",
    "                        else:\n",
    "                            hh = str(gg.iloc[0][stop_date_col_name])\n",
    "                            if hh != 'nan':\n",
    "                                stop_date = hh             \n",
    "\n",
    "                else:\n",
    "                    stop_date = row[stop_date_col_name]\n",
    "\n",
    "\n",
    "            #print('pid='+pid+', etype='+etype+', start='+str(start_date)+',  stop='+str(stop_date)+'.')\n",
    "\n",
    "            # \"Eligible\" field can be used for arbitrary filtering ahead of time, such as in Excel. Only allow rows with \"Yes\"for Eligible.\n",
    "            if row[\"Eligible\"] != \"Yes\":\n",
    "                row_is_good = False\n",
    "\n",
    "            # disease_must_match_field screens out events not related to our current disease.\n",
    "            if disease_must_match_field is not None:\n",
    "                disease_of_this_event = str(row[disease_must_match_field])\n",
    "                if disease_of_this_event == diseaseChosen or disease_of_this_event == diseaseChosen+\" Cancer\" or disease_of_this_event == diseaseChosen+\" CA\" or disease_of_this_event+\" Cancer\" == diseaseChosen :\n",
    "                    pass\n",
    "                else:\n",
    "                    row_is_good = False\n",
    "\n",
    "            if row_is_good:\n",
    "                new_row = [pid, \n",
    "                    process_val(etype, core_timeline_fields[0]), \n",
    "                    process_val(start_date, core_timeline_fields[1]), \n",
    "                    process_val(stop_date, core_timeline_fields[2]), \n",
    "                    ]\n",
    "                #print(\"new_row= \" +str(new_row))\n",
    "                \n",
    "                # print(\"columns....\")\n",
    "                # print(*current_table.columns, sep = \", \") \n",
    "                if \"EncECOG_Score\" in current_table.columns:\n",
    "                    # print('YES, ECOG is there.')\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(\"NO, ECOG is NOT there.\")\n",
    "                    pass\n",
    "\n",
    "                # now append all import fields\n",
    "                for ifield in import_fields:\n",
    "                    if ifield.source_name in current_table.columns:\n",
    "                        fieldval = row[ifield.source_name]\n",
    "                        if ifield.source_name == \"EncECOG_Score\":\n",
    "                            # print(\"===EncECOG_Score Seen!===\")\n",
    "                            # print('if source_name='+ifield.source_name+',  fieldval = ' + str(fieldval))\n",
    "                            pass\n",
    "                        # process raw value\n",
    "                        val = None\n",
    "                        try:\n",
    "                            val = process_val(fieldval, ifield)\n",
    "                            if ifield.source_name == \"EncECOG_Score\":\n",
    "                                # print('try source_name='+ifield.source_name+',  process_val = ' + str(fieldval))\n",
    "                                pass\n",
    "\n",
    "                        except:\n",
    "                            print(\"ERROR \"+current_lookup_field+\", \"+pid+\"   \"+str(val)+\".\")\n",
    "                            typef, value, traceback = sys.exc_info()\n",
    "                            print('Error value '+ str (value))\n",
    "                            pass\n",
    "\n",
    "                        new_row.append(val)\n",
    "                    else: \n",
    "                        pass    \n",
    "                        # TBD: log that we can't find the field\n",
    "\n",
    "                if subtype_source is not None:\n",
    "                    # print('wwww subtype_source is...')\n",
    "                    # print(subtype_source)\n",
    "                    # print(str(row))\n",
    "                    # print('GET val from subtype...')\n",
    "                    val = 'unknown'\n",
    "                    if callable(subtype_source):\n",
    "                        # print('Calling subtype_source...')\n",
    "                        global_subtype_loop_row = row\n",
    "                        val = subtype_source()\n",
    "                    else:\n",
    "                        val = str(row[subtype_source]).strip()\n",
    "                    if these_subtype_rewrites:\n",
    "                        key = val.lower()\n",
    "                        if key in these_subtype_rewrites:\n",
    "                            val = these_subtype_rewrites[key]\n",
    "                            # print(\"Rewrite Match: \"+key+\" -> \" + val)\n",
    "\n",
    "                    if safelist is not None:\n",
    "                        if val not in safelist:\n",
    "                            # print('safelist does not contain ['+val+'].')\n",
    "                            continue\n",
    "\n",
    "                    if these_subtype_groupings:\n",
    "                        lower_val = val.lower()\n",
    "                        grouping_match_found = False\n",
    "                        for regex in these_subtype_groupings:\n",
    "                            if re.search(regex, lower_val, re.IGNORECASE) is not None:\n",
    "                                val = these_subtype_groupings[regex]\n",
    "                                # print(\"Grouping Match: \"+ regex +\" -> \" + val)\n",
    "                                grouping_match_found = True\n",
    "                                break\n",
    "                        if grouping_match_found == False:\n",
    "                            val = \"Other \" + event_type\n",
    "\n",
    "                    if (val.strip() == \"\") or (val.lower()==\"nan\"):\n",
    "                        val = \"MISSING\" # TBD: add reporting of error.\n",
    "                        print(\"WARNING: missing subtype value.\")\n",
    "                    new_row.append(val)  #row[subtype_source])\n",
    "\n",
    "                # print(new_row)\n",
    "                rows_to_save.append(new_row)\n",
    "\n",
    "        # # turn list into df.\n",
    "        # new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "        # data_clinical_patient[ifield.final_name] = new_values\n",
    "        # print(data_clinical_patient.head(12))\n",
    "\n",
    "        print('column_names....')\n",
    "        print(type(column_names))\n",
    "        print(column_names)\n",
    "        print('---- end of column_names ---')\n",
    "\n",
    "        added_df = pd.DataFrame(rows_to_save, columns=column_names)\n",
    "        data_clinical_timeline_dfs[event_table_name] = added_df\n",
    "\n",
    "        data_clinical_timeline_graph_markers[event_table_name] = graph_marker\n",
    "        \n",
    "        # if len(noval_list) > 0 :\n",
    "        #     percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "        #     print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "        #     #print(str( len(noval_list) / new_values.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LAST_DATE of given field in another table.\n",
    "# given table T, dataval field F, and dateval field D. Like largest value in dateval_field \"StatusDate\", for matching_val \"Alive\" in import_field.source_name \"Status\". (Then, Transform)\n",
    "global_gg = []\n",
    "\n",
    "def import_lastdate_to_patient_table(tname, import_fields:List[ImportField], dateval_field, dataval_field, matching_val): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process lastdate for table \"+tname)\n",
    "    else:\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "        for ifield in import_fields: # really only one field here.\n",
    "\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                data_clinical_patient.insert(1, ifield.final_name, None)\n",
    "                current_lookup_field= ifield.source_name\n",
    "                print(\"Looking for matching_val [\"+matching_val+\"] tbl \" + tname+\".\"+current_lookup_field)\n",
    "                noval_list.clear()\n",
    "\n",
    "                def get_field_value(pid):\n",
    "                    global current_lookup_field, noval_list, global_gg\n",
    "\n",
    "                    # print(\"pid \" + pid + \", current_lookup_field \"+ifield.source_name+\", field \"+ dataval_field+\", matchTo \"+matching_val+\". gg size is...\")\n",
    "                    gg = current_table.loc[ (current_table['PatientId'] == str(pid))  & (current_table[dataval_field] == str(matching_val))  ]\n",
    "                    print(gg.size)\n",
    "                    \n",
    "                    if (gg.size == 0):\n",
    "                        return \"\"\n",
    "                        print(\"ERROR: pid \" + pid +\" has no Alive date.\")\n",
    "                    else:\n",
    "                        last_alive_str = gg[dateval_field].values[-1]  # <-- TBD: SHould actually loop through date strings for last date. We *might* get an out of order Alive statement, so don't assume last in list is always the latest.\n",
    "                        val = None\n",
    "                        try:\n",
    "                            val = last_alive_str \n",
    "                            val_final = process_val(val, ifield)\n",
    "                            return val_final\n",
    "                        except:\n",
    "                            print(\"ERROR \"+current_lookup_field+\", pid=\"+str(pid)+\"   val=[\"+str(val)+\"].\")\n",
    "                            #typef, value, traceback = sys.exc_info()\n",
    "                            #print('Error value '+ str (value))\n",
    "                            pass\n",
    "\n",
    "                new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "                if len(noval_list) > 0 :\n",
    "                    percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "                    print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "\n",
    "                data_clinical_patient[ifield.final_name] = new_values\n",
    "\n",
    "            else:\n",
    "                print(\"==== ERROR: There is no column '\"+ifield.source_name+\"' in '\"+tname+\"' table. Attempting lastdate. ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_patient_table():\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "    \n",
    "    print(\"\\nSTEP 3: Import columns to patient table.\")\n",
    "\n",
    "    import_fields = [\n",
    "       ImportField('PtGender', final_name='Sex') ,   #, conversion_function=None),\n",
    "       ImportField('PtBirthDate', final_name='BirthDate', type=\"DATE\" ),\n",
    "       ImportField('PtDeathDate', final_name='DeathDate', type=\"DATE\" )\n",
    "    ]\n",
    "    import_to_patient_table('Demographics', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n",
    "\n",
    "   \n",
    "    import_fields = [\n",
    "        ImportField('Status', final_name='LastAliveDate', type=\"DATE\") #, conversion_function=None )\n",
    "    ]\n",
    "    import_lastdate_to_patient_table('Status', import_fields, dataval_field=\"Status\", dateval_field=\"StatusDate\", matching_val=\"Alive\")\n",
    "    datafiles_fields['patient'].extend(import_fields)\n",
    "    \n",
    "\n",
    "    import_fields = [\n",
    "       ImportField('SocHxTobaccoType', final_name='Tobacco_Use'),\n",
    "       ImportField('SocHxTobaccoYears', final_name='Tobacco_Years' ), #SocHxAlcohol\n",
    "       ImportField('SocHxAlcohol', final_name='Alcohol_Use')\n",
    "    ]\n",
    "    import_to_patient_table('Social History', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields_list(import_fields, df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fields(name, import_fields):\n",
    "    global core_timeline_fields, datafiles_fields\n",
    "    datafiles_fields[name] = core_timeline_fields.copy()\n",
    "    datafiles_fields[name].extend(import_fields)\n",
    "    print('add_fields success, '+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_event_tables():\n",
    "    global core_timeline_fields, datafiles_fields\n",
    "    global global_subtype_loop_row\n",
    "\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "\n",
    "    print(\"\\nSTEP 4: Import to events tables.\")\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('EncECOG_Score', final_name='ECOG', type='NUMBER') ,   #, conversion_function=None),\n",
    "        ImportField('EncKPS', final_name='KPS', type='NUMBER') ,   #, conversion_function=None),\n",
    "    ]\n",
    "    import_to_event_table('timeline-encounters', 'Encounters', import_fields, event_type='ENCOUNTERS', start_date_col_name='EncDate', stop_date_col_name='EncDate', graph_marker=graph_marker_types[\"diamonds\"] )\n",
    "    add_fields('timeline-encounters', import_fields)\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('RadTxType', final_name='RadTxType' ) ,   \n",
    "        ImportField('RadTxTarget', final_name='RadTxTarget' ) ,   \n",
    "        ImportField('RadTxTotalDose', final_name='RadTxTotalDose' ) ,\n",
    "        ImportField('RADIATION', final_name='RADIATION' ) ,   \n",
    "    ]\n",
    "    import_to_event_table('timeline-radiation', 'Radiation Therapy', import_fields, event_type='RADIATION', disease_must_match_field=\"RadTxDisease\", start_date_col_name='RadTxDate', stop_date_col_name='RadTxStopDate', subtype_source=\"RadTxType\", graph_marker=graph_marker_types[\"arcs\"] )\n",
    "    add_fields('timeline-radiation', import_fields)\n",
    "\n",
    "    def clin_stage_subtype():\n",
    "        # global global_subtype_loop_row\n",
    "        if global_subtype_loop_row:\n",
    "            # print(\"global_subtype_loop_row...\")\n",
    "            # print(str(global_subtype_loop_row))\n",
    "            stageT = \"-\" if str(global_subtype_loop_row[\"ClinStageT\"])==\"nan\" else str(global_subtype_loop_row[\"ClinStageT\"] )\n",
    "            stageN = \"-\" if str(global_subtype_loop_row[\"ClinStageN\"])==\"nan\" else str(global_subtype_loop_row[\"ClinStageN\"] )\n",
    "            stageM = \"-\" if str(global_subtype_loop_row[\"ClinStageM\"])==\"nan\" else str(global_subtype_loop_row[\"ClinStageM\"] )\n",
    "            return stageT +\".\"+ stageN +\".\"+ stageM\n",
    "        else:\n",
    "            return 'unknown' \n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('ClinStageSystem', final_name='StageSystem' ) ,   \n",
    "        ImportField('ClinStageT', final_name='StageT' ) ,   \n",
    "        ImportField('ClinStageN', final_name='StageN' ) ,   \n",
    "        ImportField('ClinStageM', final_name='StageM' ) ,   \n",
    "        ImportField('ClinStageS', final_name='StageS' ) ,   \n",
    "    ]\n",
    "    print(\"======================= staging\")\n",
    "    import_to_event_table('timeline-clinicalstages', 'Clinical Stages', import_fields, event_type='CLINICALSTAGES', disease_must_match_field=\"ClinStageDisease\", start_date_col_name='ClinStageDate', stop_date_col_name='ClinStageDate', subtype_source=clin_stage_subtype, graph_marker=graph_marker_types[\"triangles\"] )\n",
    "    add_fields('timeline-clinicalstages', import_fields)\n",
    "\n",
    "\n",
    "    import_fields = [\n",
    "        # TBD: Add future fields, like  ImportField('MedDose', final_name='Dose' ) ,   \n",
    "        ImportField('MEDICALTHERAPY', final_name='MEDICALTHERAPY'),\n",
    "        ImportField('MedTxAgent', final_name='MedTxAgent' )  \n",
    "        \n",
    "    ]\n",
    "    import_to_event_table('timeline-medicaltherapy', 'Medical Therapy', import_fields, event_type='MEDICALTHERAPY', disease_must_match_field=\"MedTxDisease\", start_date_col_name='MedTxDate', stop_date_col_name='MedTxStopDate', subtype_source=\"MedTxAgent\" )\n",
    "    add_fields('timeline-medicaltherapy', import_fields)\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('STATUS', final_name='STATUS')\n",
    "    ]\n",
    "    import_to_event_table('timeline-status', 'Status', import_fields, event_type='STATUS', disease_must_match_field=\"StatusDisease\", start_date_col_name='StatusDate', stop_date_col_name='StatusDate', subtype_source=\"Status\", safelist=[\n",
    "        '1st Progression',\n",
    "        '2nd Progression',\n",
    "        '3rd Progression',\n",
    "        'Last Status Check',\n",
    "        'Recurrence',\n",
    "        'Local Recurrence',\n",
    "        'Locoregional',\n",
    "        'Metastatic Disease',\n",
    "        'New Diagnosis',\n",
    "        'Newly Diagnosed',\n",
    "        'No Evidence of Disease',\n",
    "\n",
    "        ] )\n",
    "    add_fields('timeline-status', import_fields)\n",
    "\n",
    "    # Add patient's DEATHDATE as a \"Death\" event.\n",
    "    import_status_to_event_table_from_patient('timeline-status', 'Status', import_fields, \n",
    "        event_type='STATUS', start_date_col_name='StatusDate', stop_date_col_name='StatusDate', subtype_source=\"Status\", patient_column=\"DeathDate\", final_name=\"Death\" )\n",
    "\n",
    "    # Add patient's LASTALIVEDATE as a \"LastAlive\" event.\n",
    "    import_status_to_event_table_from_patient('timeline-status', 'Status', import_fields, \n",
    "        event_type='STATUS', start_date_col_name='StatusDate', stop_date_col_name='StatusDate', subtype_source=\"Status\", patient_column=\"LastAliveDate\", final_name=\"LastAlive\" )\n",
    "\n",
    "    \n",
    "    import_fields = [\n",
    "        ImportField('PathHistology', final_name='Histology' ) ,   \n",
    "        ImportField('PathSpecimenType', final_name='SampleType' ) ,   \n",
    "        ImportField('PathStageT', final_name='StageT' ) ,   \n",
    "        ImportField('PathStageN', final_name='StageN' ) ,   \n",
    "        ImportField('PathStageM', final_name='StageM' ) ,   \n",
    "        ImportField('PathGrade', final_name='Grade' ) ,   \n",
    "        ImportField('PATHOLOGY', final_name='PATHOLOGY') \n",
    "    ]\n",
    "    import_to_event_table('timeline-pathology', 'Pathology', import_fields, \n",
    "        event_type='PATHOLOGY', start_date_col_name='ProcDate', stop_date_col_name='ProcDate', subtype_source=\"PathHistology\", graph_marker=graph_marker_types[\"triangles\"],\n",
    "        date_lookup_index=\"ProcedureId\", date_lookup_table_with_dates=\"Procedures\"\n",
    "         )\n",
    "    add_fields('timeline-pathology', import_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_types(filename):\n",
    "    rr=reversed(list(map(lambda s: s.type, datafiles_fields[filename])))  # ????? TBD: reversed?\n",
    "    q=[]\n",
    "    # if filename=='patient':\n",
    "    q.append('STRING') # for 'PatientID'\n",
    "    if 'timeline-' in filename:\n",
    "        # rr2=list(map(lambda s: s, datafiles_fields[filename]))\n",
    "        # for w in list(rr2):\n",
    "        #     print( \"sourcename> \"+str(w.source_name))\n",
    "\n",
    "        wq=list(map(lambda s: s.type, datafiles_fields[filename]))\n",
    "        print(\"in getcoltypes for \"+filename+\",...\")\n",
    "        for atype in list(wq):\n",
    "            print( \"> \"+str(atype))\n",
    "            q.append(atype)\n",
    "    else:\n",
    "        # just patient table\n",
    "        q.extend(list(rr))\n",
    "\n",
    "    if filename=='patient':\n",
    "        q.append('DATE') # for 'DiagnosisDate'\n",
    "        q.append('STRING') # for 'StatusProbability'\n",
    "\n",
    "    print(\"end getcoltypes,\")\n",
    "    print(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_as_cbioportal(df:pd.DataFrame, filename):\n",
    "    output_filename = 'data_clinical_'+filename+'.txt'\n",
    "    full_filename = dataset_folder + \"/\"+foldername_with_headers+\"/\" + output_filename\n",
    "    if not os.path.exists(os.path.dirname(full_filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(full_filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(full_filename, 'w', newline='\\r\\n', encoding=\"utf-8\") as f:\n",
    "        col_names_raw = list(df.columns.values)   # was data_clinical_patient.columns.values\n",
    "        col_names_raw[0] = \"PATIENT_ID\" # Hack to insert underscore\n",
    "        print('col_names_raw...')\n",
    "        print(col_names_raw)\n",
    "\n",
    "        col_names = [x.upper() for x in col_names_raw]\n",
    "        col_types = get_col_types(filename)\n",
    "        \n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\") # header 1, internal name\n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\")  # header 2, description\n",
    "        f.write('#' + '\\t'.join(col_types) + \"\\n\")  # header 3, type (STRING or NUMBER)  <<<<<\n",
    "        f.write('#' + '\\t'.join(['1'] * len(col_names)) + \"\\n\")  # header 4, position\n",
    "        f.write('\\t'.join(col_names) + \"\\n\")  # header 5, readable name\n",
    "\n",
    "        for row in df.iterrows():\n",
    "            output_row = []\n",
    "            row_as_list = list(row[1])\n",
    "            i=0\n",
    "\n",
    "            for item in row_as_list:\n",
    "                cleaned_item = str(item)\n",
    "                # print('--> '+cleaned_item)\n",
    "                if item == None:\n",
    "                    cleaned_item = \"\"\n",
    "                else:\n",
    "                    if 'time' in str(type(item)):   # datetime.datetime or pandas...timestamp.Timestamp # this does not work:  col_types[i]==\"DATE\":  # \n",
    "                        # print('DATE for col ' + str(i) +', '+str(col_names[i]))\n",
    "                        cleaned_item = datetime.strftime(item, date_format) # '%Y-%M-%d')\n",
    "                if (str(item)==\"nan\"):\n",
    "                    cleaned_item = \"\"\n",
    "                output_row.append(cleaned_item)\n",
    "                i=i+1\n",
    "\n",
    "            items_to_str = '\\t'.join(output_row).replace('\"', ' ')\n",
    "            f.write(items_to_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_empty_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory : \", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", dir)  \n",
    "        import shutil\n",
    "        for root, dirs, files in os.walk(dir):\n",
    "            for f in files:\n",
    "                os.unlink(os.path.join(root, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files_as_cbioportal():\n",
    "    # clear out the folder, if it has files\n",
    "    dir = dataset_folder + \"/\"+foldername_with_headers+\"/\" \n",
    "    create_and_empty_folder(dir)\n",
    "\n",
    "    print('Write patient meta file...')\n",
    "\n",
    "    filename = 'patient'\n",
    "    write_file_as_cbioportal(data_clinical_patient, filename)\n",
    "    \n",
    "    # loop through all table files\n",
    "    for df_key in data_clinical_timeline_dfs.keys():\n",
    "        print('Attempt to write '+df_key+'.')\n",
    "        write_file_as_cbioportal( data_clinical_timeline_dfs[df_key], df_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guts of processing\n",
    "find_patients_first_dx()\n",
    "print(patients_first_dx.shape)\n",
    "load_all_tables()\n",
    "datafiles_fields = {\n",
    "    'patient': []   #not including PATIENT_ID\n",
    "}\n",
    "import_fields_to_patient_table()\n",
    "import_fields_to_event_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files_as_cbioportal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_files(dir):\n",
    "    # Assumes dir already created, with create_and_empty_folder().\n",
    "\n",
    "    full_filename = dir+\"/\" + \"meta_study.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"type_of_cancer: \"+ type_of_cancer +\"\\n\") \n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"name: \"+ study_identifier +\"\\n\") \n",
    "        f.write(\"short_name: \"+ study_identifier +\"\\n\") \n",
    "        f.write(\"description: \"+ study_description +\"\\n\") \n",
    "        f.write(\"add_global_case_list: true\" +\"\\n\") \n",
    "\n",
    "    full_filename = dir+\"/\" + \"meta_clinical_patient.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "        f.write(\"datatype: PATIENT_ATTRIBUTES\\n\") \n",
    "        f.write(\"data_filename: data_clinical_patient.txt\" +\"\\n\") \n",
    "\n",
    "    # ==== START SAMPLE FILE =====\n",
    "    # Oncoscape expects a sample/specimen file. Currently we don't support importing them, \n",
    "    # so for now make a dummy file with a one-to-one mapping of patientid with \"sample-<patientid>\".\n",
    "    full_filename = dir+\"/\" + \"meta_clinical_specimen_placeholder.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "        f.write(\"datatype: SAMPLE_ATTRIBUTES\\n\") \n",
    "        f.write(\"data_filename: data_clinical_specimen_placeholder.txt\" +\"\\n\") \n",
    "\n",
    "    full_filename = dir+\"/\" + \"data_clinical_specimen_placeholder.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"#Unique_patient_identifier\tSPECIMEN_ID\\n\")\n",
    "        f.write(\"#STRING\tSTRING\\n\")\n",
    "        f.write(\"#1\t1\\n\")\n",
    "        f.write(\"PATIENT_ID\tSPECIMEN_ID\\n\")\n",
    "        for row in data_clinical_patient.iterrows():\n",
    "            f.write(row[1]['PatientId'] +\"\\tsample-\" + row[1]['PatientId']+\"\\n\")\n",
    "    # ==== END SAMPLE FILE =====\n",
    "\n",
    "    for event_table_name in data_clinical_timeline_dfs.keys():\n",
    "        full_filename = dir+\"/\" + \"meta_clinical_\"+event_table_name+\".txt\"\n",
    "        with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "            f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "            f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "            f.write(\"datatype: TIMELINE\\n\") \n",
    "            f.write(\"data_filename: data_clinical_\"+ event_table_name+\".txt\" +\"\\n\") \n",
    "            graph_marker = data_clinical_timeline_graph_markers[event_table_name]\n",
    "            if graph_marker is None:\n",
    "                # f.write('oncoscape_bar_override: {\"version\": \"1.0\", \"style\": \"Bars\", \"shape\": \"circle\", \"subtypeColors\": {\"rp\": \"#FF0000\", \"xrt\": \"#00FF00\"} }')\n",
    "                pass\n",
    "            else:\n",
    "                label = event_table_name\n",
    "                if label.startswith('timeline-'):\n",
    "                    label = label[len('timeline-'):]\n",
    "\n",
    "                bar_override = 'oncoscape_bar_override: {\"version\": \"1.0\", \"label\": \"'+label+'\" '\n",
    "                shape_extender = \"\"\n",
    "                print(\"===========  graph_marker_key ===========\")\n",
    "                print(str(graph_marker))\n",
    "                if graph_marker[\"shape\"] is not None:\n",
    "                    shape_extender = ', \"shape\": \"'+graph_marker[\"shape\"]+'\" '\n",
    "                bar_override = bar_override + ', \"style\": \"'+graph_marker[\"mtype\"]+'\" ' + shape_extender\n",
    "                bar_override = bar_override + ' }'\n",
    "                \n",
    "\n",
    "                f.write(bar_override)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = dataset_folder + \"/\"+foldername_zero_dates+\"/\" \n",
    "create_and_empty_folder(dir)\n",
    "zero_dates(dataset_folder+'/'+foldername_with_headers, patient_info_filename = \"data_clinical_patient.txt\", zero_day_column_name =  \"DIAGNOSISDATE\", output_folder=dir )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_meta_files(dir) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
